{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ae21380",
   "metadata": {},
   "source": [
    "## CA 2 - Big Data & Advanced Analytics\n",
    "\n",
    "### Using Pyspark for data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "baea30d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear the cache on the spark session\n",
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8b823f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# what version of pyspark is running on the computer using SparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4ce1fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sc master - running locally\n",
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ff8a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import regex module\n",
    "import re\n",
    "from operator import add\n",
    "\n",
    "# Import Pyspark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Other Libraries \n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f720fb",
   "metadata": {},
   "source": [
    "#### Import File from the Hadoop Directory\n",
    "\n",
    "<b>References</b>\n",
    "1. https://sparkbyexamples.com/pyspark/pyspark-read-csv-file-into-dataframe/\n",
    "2. https://medium.com/@ashutoshkumar2048/spark-connect-apache-spark-3-4-9846c40484d0\n",
    "3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853ba5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the spark session for CA2\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ca2_V2\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5269d216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a structure to hold the data, name and define data types\n",
    "schema = StructType([\n",
    "    StructField(\"target\", IntegerType(), True),\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"query\", StringType(), True),\n",
    "    StructField(\"author\", StringType(), True),\n",
    "    StructField(\"tweet\", StringType(), True),\n",
    "    StructField(\"processed_tweet\", StringType(), True)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86ac3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read input file from hadoop directory on the local drive\n",
    "# import the csv file from hadoop\n",
    "path = \"/user1/twitter_DS_1yr.csv\"\n",
    "\n",
    "df = spark.read.csv(path, header=False, inferSchema=True, schema=schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f93f8b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert 'date' to a date type\n",
    "\n",
    "df.select(col('date'), to_date(col('date'), 'MM-DD').alias('date_split'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbae2b9",
   "metadata": {},
   "source": [
    "#### Exploring the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b95de54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of rows\n",
    "num_rows = df.count()\n",
    "\n",
    "# Get the number of columns\n",
    "num_columns = len(df.columns)\n",
    "\n",
    "# Print the shape\n",
    "print(\"Number of rows: \", num_rows)\n",
    "print(\"Number of columns: \", num_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c99b519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at types of values in the polarity\n",
    "\n",
    "print(f\"There is {df[df['target']==4].count()} positive values in the dataframe.\")\n",
    "\n",
    "print(f\"There is {df[df['target']==0].count()} negative values in the dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b283edd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate entries\n",
    "\n",
    "df = df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e75a744",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop any n/a rows\n",
    "\n",
    "df.dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3698fd2c",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd47785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libaries\n",
    "\n",
    "from pyspark.sql.functions import udf, regexp_replace, lower\n",
    "from pyspark.sql.types import StringType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29e89e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove noise such as html links, stop words / punctuation / #hashtags etc\n",
    "# REFERENCE: https://medium.com/towards-artificial-intelligence/large-scale-sentiment-analysis-with-pyspark-bdccf9256e35\n",
    "\n",
    "def pre_process(text):\n",
    "    # Remove links\n",
    "    #text = re.sub('http://\\S+|https://\\S+', '', text)\n",
    "    #text = re.sub('http[s]?://\\S+', '', text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "\n",
    "    # Convert HTML references\n",
    "    text = re.sub(r'&amp', 'and', text)\n",
    "    text = re.sub(r'&lt', '<', text)\n",
    "    text = re.sub(r'&gt', '>', text)\n",
    "    #text = re.sub(' ', text)\n",
    "\n",
    "    # Remove new line characters\n",
    "    text = re.sub(r'[\\r\\n]+', ' ', text)\n",
    "    \n",
    "    # Remove mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # Remove hashtags\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "\n",
    "    # Remove multiple space characters\n",
    "    text = re.sub(r'\\s+',' ', text)\n",
    "    \n",
    "    # Convert all text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a9d203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the pre_process function as a UDF (User-Defined Function)\n",
    "\n",
    "pre_process_udf = udf(pre_process, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0f39b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the UDF to the 'Tweet_details' column and create a new column 'Processed_tweet_details'\n",
    "\n",
    "df = df.withColumn('processed_tweet', pre_process_udf('tweet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0415dc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate entries\n",
    "\n",
    "df = df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba741d85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the DataFrame with the new column\n",
    "\n",
    "#df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a21abf5",
   "metadata": {},
   "source": [
    "#### Feature Extraction\n",
    "\n",
    "<b>References</b>\n",
    "1. https://medium.com/towards-artificial-intelligence/large-scale-sentiment-analysis-with-pyspark-bdccf9256e35\n",
    "\n",
    "2. https://medium.com/@chris_42047/an-easy-guide-to-basic-twitter-sentiment-analysis-python-tutorial-1630d5213ff6\n",
    "\n",
    "3. https://www.kaggle.com/code/muhammetzahitaydn/pyspark-sentiment-analysis-with-word2vec-embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96712e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the relevant libraries tocreate a pipleline\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StringIndexer, CountVectorizer, NGram, VectorAssembler, ChiSqSelector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a3c160f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tokenizer \n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"processed_tweet\", outputCol=\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac2a4907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HashingTF: Hashing Term Frequency\n",
    "# REFERENCE - https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.HashingTF.html#pyspark.ml.feature.HashingTF\n",
    "# Maps a sequence of terms to their term frequencies using the hashing\n",
    "\n",
    "hashtf = HashingTF(inputCol=\"words\", outputCol='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c09199a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the hashtf function to the IDF function\n",
    "# REFERENCE - https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.IDF.html#pyspark.ml.feature.IDF\n",
    "# Compute the Inverse Document Frequency (IDF) given a collection of documents ie the tweets df\n",
    "\n",
    "idf = IDF(inputCol='tf', outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e10785f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index labels\n",
    "\n",
    "label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ff66134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, hashtf, idf, label_stringIdx, lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60630ff3",
   "metadata": {},
   "source": [
    "#### Filter data based on key word - 'weather'\n",
    "\n",
    "##### REFERENCES \n",
    "1. https://towardsdatascience.com/sentiment-analysis-with-pyspark-bc8e83f80c35\n",
    "2. https://towardsdatascience.com/my-absolute-go-to-for-sentiment-analysis-textblob-3ac3a11d524"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81cc0d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a variable to search within the dataframe on\n",
    "\n",
    "search_for = \"weather\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bab3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.filter(df[\"processed_tweet\"].contains(search_for))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6822572",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b4b336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove unnecessary columns \n",
    "\n",
    "df2.drop('query','author', 'tweet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72d66b0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the number of rows in the new df\n",
    "num_rows = df2.count()\n",
    "\n",
    "# Get the number of columns\n",
    "num_columns = len(df2.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b058cdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the shape\n",
    "print(\"Number of rows: \", num_rows)\n",
    "print(\"Number of columns: \", num_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80441449",
   "metadata": {},
   "source": [
    "### **************************************************************\n",
    "\n",
    "\n",
    "\n",
    "#### Sentiment extraction using TextBlob\n",
    "\n",
    "##### REFERENCES\n",
    "\n",
    "1. https://towardsdatascience.com/my-absolute-go-to-for-sentiment-analysis-textblob-3ac3a11d524\n",
    "2. https://towardsdatascience.com/sentiment-analysis-with-pyspark-bc8e83f80c35\n",
    "3. https://towardsdatascience.com/my-absolute-go-to-for-sentiment-analysis-textblob-3ac3a11d524"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dce68307",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from textblob import TextBlob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d15a73fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a UDF to apply sentiment analysis using TextBlob\n",
    "\n",
    "def get_sentiment(processed_tweet):\n",
    "    analysis = TextBlob(processed_tweet)\n",
    "    \n",
    "    sentiment = analysis.sentiment.polarity\n",
    "\n",
    "    return sentiment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cd93a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Register the UDF\n",
    "get_sentiment_udf = spark.udf.register(\"get_sentiment\", get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af564edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the UDF to the 'tweet' column and create a new column 'sentiment'\n",
    "df2 = df2.withColumn('sentiment', get_sentiment_udf('processed_tweet'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee9a853",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Show the DataFrame with the 'processed_tweet' and 'sentiment' columns\n",
    "df2.select('processed_tweet', 'sentiment').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34346aaf",
   "metadata": {},
   "source": [
    "#### Save Weather Data to a new Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dc0f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REFERENCE - https://sparkbyexamples.com/pyspark/pyspark-write-dataframe-to-csv-file/\n",
    "\n",
    "#rename the dataframe\n",
    "\n",
    "weather_tweets2 = df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05685e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the file to a csv file\n",
    "\n",
    "#weather_tweets.write.csv(\"hdfs://localhost:9000/user1/weather_tweets2\")\n",
    "\n",
    "# commented out as file already exists on Hadop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5062fb1e",
   "metadata": {},
   "source": [
    "### Time Series\n",
    "\n",
    "##### References\n",
    "\n",
    "1. https://towardsdatascience.com/end-to-end-time-series-interpolation-in-pyspark-filling-the-gap-5ccefc6b7fc9\n",
    "2. https://medium.com/@y.s.yoon/scalable-time-series-forecasting-in-spark-prophet-cnn-lstm-and-sarima-a5306153711e\n",
    "3. https://medium.com/delaware-pro/interpolate-big-data-time-series-in-native-pyspark-d270d4b592a1\n",
    "4. https://www.kaggle.com/code/qingyi/time-series-data-analysis-with-spark\n",
    "5. https://www.analyticsvidhya.com/blog/2022/01/apache-spark-and-facebook-prophet/\n",
    "6. \n",
    "****\n",
    "Fastai is a deep learning library that was used to complete the analsysis.  This was the selected algorithm as it can be used with pyspark.  Finally the library can work with tabular and text data so can be used to complete a time series and a sentiment analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e1c3f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import pandas_udf, PandasUDFType\n",
    "from pyspark.sql import SparkSession\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "import pandas as pd\n",
    "\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql import functions as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7faea477",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "426e7322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split date column by characters based on year / month / day\n",
    "weather_tweets2  = weather_tweets2.withColumn('Day', F.substring('date', 1, 4))\n",
    "weather_tweets2  = weather_tweets2.withColumn('Month', F.substring('date', 5, 3))\n",
    "weather_tweets2  = weather_tweets2.withColumn('Month_date', F.substring('date', 8, 2))\n",
    "\n",
    "# split date column by characters based on hour / minute /second\n",
    "#weather_tweets2  = weather_tweets2.withColumn('hour', F.substring('date', 10, 2))\n",
    "#weather_tweets2  = weather_tweets2.withColumn('minute', F.substring('date', 12, 2))\n",
    "#weather_tweets2  = weather_tweets2.withColumn('second', F.substring('date', 14, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ab716f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert weather_tweets 2 from a spark df to a Pandas df for processing\n",
    "pdf = weather_tweets2.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7821d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdf2 = pdf.drop(['id', 'query', 'author', 'tweet'], axis =1)\n",
    "\n",
    "pdf2 = pdf.drop(['id', 'query', 'author', 'tweet', 'Month', 'Month_date', 'Day'], axis =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cab52d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove 'PDT' from pdf2 \n",
    "\n",
    "pdf2['date'] = pdf2['date'].astype(str).str.replace(' PDT', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6e7daf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf2['date'] = pd.to_datetime(pdf2['date'].str.replace(' PDT', ''), errors='coerce', utc=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be224995",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the data in chronological order by date\n",
    "\n",
    "pdf2 = pdf2.sort_values('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e325a495",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pdf2.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f3acba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the index to the pdf2 based on the date column\n",
    "\n",
    "pdf2.set_index('date', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0e3c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pdf.info()\n",
    "\n",
    "pdf2.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0da135c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c30185f2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Define a pandas UDF to apply ARIMA model\n",
    "\n",
    "@pandas_udf('double', PandasUDFType.SCALAR)\n",
    "def forecast_arima(time_series: pd.Series) -> pd.Series:\n",
    "    \n",
    "    model = ARIMA(time_series, order=(5,1,0))\n",
    "    model_fit = model.fit()\n",
    "   \n",
    "    #forecast = model_fit.forecast(steps=1)[0]\n",
    "    forecast_output = model_fit.forecast(steps=1)\n",
    "    print(forecast_output)\n",
    "    forecast = forecast_output[0]\n",
    "\n",
    "    \n",
    "    return pd.Series(forecast)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd476c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c34fb2ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the UDF to the 'target' column, not 'date'\n",
    "\n",
    "df = df.withColumn('forecast', forecast_arima(col('target')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d909e12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c7bb758",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d38ccb17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c86ee56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87735b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the relevant libraries tocreate a pipleline\n",
    "\n",
    "#from pyspark.ml import Pipeline\n",
    "#from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StringIndexer, CountVectorizer, NGram, VectorAssembler, ChiSqSelector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aed335a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tokenizer \n",
    "\n",
    "#tokenizer = Tokenizer(inputCol=\"processed_tweet\", outputCol=\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965b456b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HashingTF: Hashing Term Frequency\n",
    "# REFERENCE - https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.HashingTF.html#pyspark.ml.feature.HashingTF\n",
    "# Maps a sequence of terms to their term frequencies using the hashing\n",
    "\n",
    "#hashtf = HashingTF(inputCol=\"words\", outputCol='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072f8d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the hashtf function to the IDF function\n",
    "# REFERENCE - https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.IDF.html#pyspark.ml.feature.IDF\n",
    "# Compute the Inverse Document Frequency (IDF) given a collection of documents ie the tweets df\n",
    "\n",
    "#idf = IDF(inputCol='tf', outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e798dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index labels\n",
    "\n",
    "#label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fdc4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "#lr = LogisticRegression()\n",
    "\n",
    "#pipeline = Pipeline(stages=[tokenizer, hashtf, idf, label_stringIdx, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9cd3e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6920f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
