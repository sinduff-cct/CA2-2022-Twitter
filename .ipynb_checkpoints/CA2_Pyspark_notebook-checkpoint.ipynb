{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ae21380",
   "metadata": {},
   "source": [
    "## CA 2 - Big Data & Advanced Analytics\n",
    "\n",
    "### Using Pyspark for data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baea30d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear the cache on the spark session\n",
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8b823f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what version of pyspark is running on the computer using SparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4ce1fe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local[*]'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sc master - running locally\n",
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16ff8a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import regex module\n",
    "import re\n",
    "from operator import add\n",
    "\n",
    "# Import Pyspark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Other Libraries \n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f720fb",
   "metadata": {},
   "source": [
    "#### Import File from the Hadoop Directory\n",
    "\n",
    "<b>References</b>\n",
    "1. https://sparkbyexamples.com/pyspark/pyspark-read-csv-file-into-dataframe/\n",
    "2. https://medium.com/@ashutoshkumar2048/spark-connect-apache-spark-3-4-9846c40484d0\n",
    "3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "853ba5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the spark session for CA2\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ca2\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5269d216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a structure to hold the data, name and define data types\n",
    "schema = StructType([\n",
    "    StructField(\"target\", IntegerType(), True),\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"query\", StringType(), True),\n",
    "    StructField(\"author\", StringType(), True),\n",
    "    StructField(\"tweet\", StringType(), True)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d86ac3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read input file from hadoop directory on the local drive\n",
    "# import the csv file from hadoop\n",
    "path = \"/user1/twitter_DS_1yr.csv\"\n",
    "\n",
    "df = spark.read.csv(path, header=False, inferSchema=True, schema=schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cc39a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import check - show the first 10 rows of the imported table\n",
    "\n",
    "#df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbae2b9",
   "metadata": {},
   "source": [
    "#### Exploring the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b95de54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows:  1600000\n",
      "Number of columns:  6\n"
     ]
    }
   ],
   "source": [
    "# Get the number of rows\n",
    "#num_rows = df.count()\n",
    "\n",
    "# Get the number of columns\n",
    "#num_columns = len(df.columns)\n",
    "\n",
    "# Print the shape\n",
    "#print(\"Number of rows: \", num_rows)\n",
    "#print(\"Number of columns: \", num_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c99b519",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 800000 positive values in the dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 800000 negative values in the dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# look at types of values in the polarity\n",
    "\n",
    "#print(f\"There is {df[df['target']==4].count()} positive values in the dataframe.\")\n",
    "#\n",
    "#print(f\"There is {df[df['target']==0].count()} negative values in the dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b283edd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate entries\n",
    "\n",
    "df = df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4357a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3698fd2c",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffd47785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libaries\n",
    "\n",
    "from pyspark.sql.functions import udf, regexp_replace, lower\n",
    "from pyspark.sql.types import StringType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e75a744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[target: int, id: string, date: string, query: string, author: string, tweet: string]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop any n/a rows\n",
    "\n",
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29e89e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove noise such as html links, stop words / punctuation / #hashtags etc\n",
    "# REFERENCE: https://medium.com/towards-artificial-intelligence/large-scale-sentiment-analysis-with-pyspark-bdccf9256e35\n",
    "\n",
    "def pre_process(text):\n",
    "    # Remove links\n",
    "    #text = re.sub('http://\\S+|https://\\S+', '', text)\n",
    "    #text = re.sub('http[s]?://\\S+', '', text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "\n",
    "    # Convert HTML references\n",
    "    text = re.sub(r'&amp', 'and', text)\n",
    "    text = re.sub(r'&lt', '<', text)\n",
    "    text = re.sub(r'&gt', '>', text)\n",
    "    #text = re.sub(' ', text)\n",
    "\n",
    "    # Remove new line characters\n",
    "    text = re.sub(r'[\\r\\n]+', ' ', text)\n",
    "    \n",
    "    # Remove mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # Remove hashtags\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "\n",
    "    # Remove multiple space characters\n",
    "    text = re.sub(r'\\s+',' ', text)\n",
    "    \n",
    "    # Convert all text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15a9d203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the pre_process function as a UDF (User-Defined Function)\n",
    "\n",
    "pre_process_udf = udf(pre_process, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0f39b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the UDF to the 'Tweet_details' column and create a new column 'Processed_tweet_details'\n",
    "\n",
    "df = df.withColumn('processed_tweet', pre_process_udf('tweet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba741d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+--------+---------------+--------------------+--------------------+\n",
      "|target|        id|                date|   query|         author|               tweet|     processed_tweet|\n",
      "+------+----------+--------------------+--------+---------------+--------------------+--------------------+\n",
      "|     0|1468106999|Mon Apr 06 23:45:...|NO_QUERY|    stuartheron|@watko Shockingly...|    shockingly not! |\n",
      "|     0|1686953810|Sun May 03 07:19:...|NO_QUERY|forevaguitargrl|up ad ready for c...|up ad ready for c...|\n",
      "|     0|1956161267|Thu May 28 21:18:...|NO_QUERY|      SupaSista|Someone just sent...|someone just sent...|\n",
      "|     0|1956910895|Thu May 28 23:00:...|NO_QUERY| jeanettiewuvsu|Wtf is wrong with...|wtf is wrong with...|\n",
      "|     0|1963373312|Fri May 29 12:37:...|NO_QUERY|        XkyRauh|Only two weeks le...|only two weeks le...|\n",
      "|     0|1972460907|Sat May 30 09:16:...|NO_QUERY|         jiaaaa|You gave me false...|you gave me false...|\n",
      "|     0|1973029187|Sat May 30 10:20:...|NO_QUERY|   pinkkstarzzz|Still haven't got...|still haven't got...|\n",
      "|     0|1973874319|Sat May 30 12:00:...|NO_QUERY|       hopealot|@thejasonrobison ...| have fun!! i mis...|\n",
      "|     0|1974232831|Sat May 30 12:42:...|NO_QUERY|   christine426|I am craving crio...|i am craving crio...|\n",
      "|     0|1979507441|Sun May 31 03:20:...|NO_QUERY|       IRISMORE|@XChadballX tonig...| tonight , is new...|\n",
      "+------+----------+--------------------+--------+---------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the DataFrame with the new column\n",
    "\n",
    "#df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a21abf5",
   "metadata": {},
   "source": [
    "#### Feature Extraction\n",
    "\n",
    "<b>References</b>\n",
    "1. https://medium.com/towards-artificial-intelligence/large-scale-sentiment-analysis-with-pyspark-bdccf9256e35\n",
    "\n",
    "2. https://medium.com/@chris_42047/an-easy-guide-to-basic-twitter-sentiment-analysis-python-tutorial-1630d5213ff6\n",
    "\n",
    "3. https://www.kaggle.com/code/muhammetzahitaydn/pyspark-sentiment-analysis-with-word2vec-embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96712e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the relevant libraries tocreate a pipleline\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StringIndexer, CountVectorizer, NGram, VectorAssembler, ChiSqSelector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a3c160f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tokenizer \n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"processed_tweet\", outputCol=\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac2a4907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HashingTF: Hashing Term Frequency\n",
    "# REFERENCE - https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.HashingTF.html#pyspark.ml.feature.HashingTF\n",
    "# Maps a sequence of terms to their term frequencies using the hashing\n",
    "\n",
    "hashtf = HashingTF(inputCol=\"words\", outputCol='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c09199a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the hashtf function to the IDF function\n",
    "# REFERENCE - https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.IDF.html#pyspark.ml.feature.IDF\n",
    "# Compute the Inverse Document Frequency (IDF) given a collection of documents ie the tweets df\n",
    "\n",
    "idf = IDF(inputCol='tf', outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e10785f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index labels\n",
    "\n",
    "label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ff66134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, hashtf, idf, label_stringIdx, lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60630ff3",
   "metadata": {},
   "source": [
    "#### Filter data based on key word - 'weather'\n",
    "\n",
    "##### REFERENCES \n",
    "1. https://towardsdatascience.com/sentiment-analysis-with-pyspark-bc8e83f80c35\n",
    "2. https://towardsdatascience.com/my-absolute-go-to-for-sentiment-analysis-textblob-3ac3a11d524"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81cc0d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a variable to search within the dataframe on\n",
    "\n",
    "search_for = \"weather\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5bab3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.filter(df[\"processed_tweet\"].contains(search_for))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6822572",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:=====================================================>(199 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+--------+---------------+--------------------+--------------------+\n",
      "|target|        id|                date|   query|         author|               tweet|     processed_tweet|\n",
      "+------+----------+--------------------+--------+---------------+--------------------+--------------------+\n",
      "|     4|1964587188|Fri May 29 14:28:...|NO_QUERY|       SuperRob|@sparktography We...| weather.com says...|\n",
      "|     0|2264778973|Sun Jun 21 04:55:...|NO_QUERY|    deirdre7194|@Es94 yay!! i did...| yay!! i didnt no...|\n",
      "|     4|2060752623|Sat Jun 06 19:30:...|NO_QUERY|     HailHorror|Finishing my pack...|finishing my pack...|\n",
      "|     0|2229093418|Thu Jun 18 15:16:...|NO_QUERY| nocheapthrillz|@afreshmusic hey ...| hey d! my bad, i...|\n",
      "|     0|2191448262|Tue Jun 16 05:17:...|NO_QUERY|     tippielove|PS for those intr...|ps for those intr...|\n",
      "|     0|2222121192|Thu Jun 18 06:31:...|NO_QUERY|  abitheamazing|so its nice weath...|so its nice weath...|\n",
      "|     0|1992186304|Mon Jun 01 08:20:...|NO_QUERY|     CalmedFury|Why did the weath...|why did the weath...|\n",
      "|     4|2001516219|Tue Jun 02 01:08:...|NO_QUERY|       Renatuuh|I just love long ...|i just love long ...|\n",
      "|     4|1988818733|Sun May 31 23:27:...|NO_QUERY|stroughtonsmith|@MadMax you'd fin...| you'd find other...|\n",
      "|     4|1793480642|Thu May 14 02:51:...|NO_QUERY|      winterdew|Well, it will hit...|well, it will hit...|\n",
      "+------+----------+--------------------+--------+---------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#df2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "72d66b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Get the number of rows in the new df#\n",
    "#num_rows = df2.count()\n",
    "\n",
    "# Get the number of columns\n",
    "#num_columns = len(df2.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b058cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows:  10342\n",
      "Number of columns:  7\n"
     ]
    }
   ],
   "source": [
    "# Print the shape\n",
    "#print(\"Number of rows: \", num_rows)\n",
    "#print(\"Number of columns: \", num_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80441449",
   "metadata": {},
   "source": [
    "### **************************************************************\n",
    "\n",
    "\n",
    "\n",
    "#### Sentiment extraction using TextBlob\n",
    "\n",
    "##### REFERENCES\n",
    "\n",
    "1. https://towardsdatascience.com/my-absolute-go-to-for-sentiment-analysis-textblob-3ac3a11d524\n",
    "2. https://towardsdatascience.com/sentiment-analysis-with-pyspark-bc8e83f80c35\n",
    "3. https://towardsdatascience.com/my-absolute-go-to-for-sentiment-analysis-textblob-3ac3a11d524"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dce68307",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from textblob import TextBlob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d15a73fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a UDF to apply sentiment analysis using TextBlob\n",
    "\n",
    "def get_sentiment(processed_tweet):\n",
    "    analysis = TextBlob(processed_tweet)\n",
    "    \n",
    "    sentiment = analysis.sentiment.polarity\n",
    "\n",
    "    return sentiment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1cd93a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Register the UDF\n",
    "get_sentiment_udf = spark.udf.register(\"get_sentiment\", get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "af564edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the UDF to the 'tweet' column and create a new column 'sentiment'\n",
    "df = df.withColumn('sentiment', get_sentiment_udf('processed_tweet'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3ee9a853",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 16:>                                                         (0 + 1) / 2]\r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_5705/3698233587.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Show the DataFrame with the 'tweet' and 'sentiment' columns\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'processed_tweet'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'sentiment'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/usr/local/spark/python/pyspark/sql/dataframe.py\u001b[0m in \u001b[0;36mshow\u001b[0;34m(self, n, truncate, vertical)\u001b[0m\n\u001b[1;32m    484\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    485\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 486\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshowString\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtruncate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvertical\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    487\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    488\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1301\u001b[0m             \u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEND_COMMAND_PART\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1303\u001b[0;31m         \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgateway_client\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1304\u001b[0m         return_value = get_return_value(\n\u001b[1;32m   1305\u001b[0m             answer, self.gateway_client, self.target_id, self.name)\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m         \u001b[0mconnection\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_connection\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1032\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1033\u001b[0;31m             \u001b[0mresponse\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconnection\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msend_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcommand\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1034\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1035\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresponse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_connection_guard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconnection\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/spark/python/lib/py4j-0.10.9-src.zip/py4j/java_gateway.py\u001b[0m in \u001b[0;36msend_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m   1198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1199\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1200\u001b[0;31m             \u001b[0manswer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmart_decode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1201\u001b[0m             \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Answer received: {0}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0manswer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1202\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0manswer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mproto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRETURN_MESSAGE\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/lib/python3.10/socket.py\u001b[0m in \u001b[0;36mreadinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 705\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sock\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecv_into\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    706\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    707\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_timeout_occurred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# Show the DataFrame with the 'tweet' and 'sentiment' columns\n",
    "df.select('processed_tweet', 'sentiment').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181dc9de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34346aaf",
   "metadata": {},
   "source": [
    "#### Save Weather Data to a new Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17dc0f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REFERENCE - https://sparkbyexamples.com/pyspark/pyspark-write-dataframe-to-csv-file/\n",
    "\n",
    "#rename the dataframe\n",
    "\n",
    "weather_tweets2 = df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a05685e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the file to a csv file\n",
    "\n",
    "#weather_tweets.write.csv(\"hdfs://localhost:9000/user1/weather_tweets2\")\n",
    "\n",
    "# commented out as file already exists on Hadop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5062fb1e",
   "metadata": {},
   "source": [
    "### Time Series\n",
    "\n",
    "##### References\n",
    "\n",
    "1. https://towardsdatascience.com/end-to-end-time-series-interpolation-in-pyspark-filling-the-gap-5ccefc6b7fc9\n",
    "2. https://medium.com/@y.s.yoon/scalable-time-series-forecasting-in-spark-prophet-cnn-lstm-and-sarima-a5306153711e\n",
    "3. https://medium.com/delaware-pro/interpolate-big-data-time-series-in-native-pyspark-d270d4b592a1\n",
    "4. https://www.fast.ai/    and   https://github.com/fastai\n",
    "6. https://docs.fast.ai/tutorial.text.html\n",
    "\n",
    "****\n",
    "Fastai is a deep learning library that was used to complete the analsysis.  This was the selected algorithm as it can be used with pyspark.  Finally the library can work with tabular and text data so can be used to complete a time series and a sentiment analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89272d77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import Libraries \n",
    "\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from fastai.tabular import *\n",
    "import six\n",
    "\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DateType, IntegerType, FloatType\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69b0c211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the data in chronological order by date\n",
    "\n",
    "weather_tweets2 = weather_tweets2.orderBy('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0c8765d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert weather_tweets 2 from a spark df to a Pandas df for processing\n",
    "pdf = weather_tweets2.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "769189f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# display the resulting pandas dataframe\n",
    "\n",
    "pdf.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dd2267",
   "metadata": {},
   "source": [
    "#### Preparing for the ML Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddeb47b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare to use fastai library to complete the analysis \n",
    "# Step 1 - defining variables fpr the analysis\n",
    "# REFERNCE - https://docs.fast.ai/quick_start.html\n",
    "\n",
    "\n",
    "target_var = 'processed_tweet'\n",
    "categorical_vars = ['target', 'author']\n",
    "date_var = ['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2925d91a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a list of preprocessing operations used on the data for training\n",
    "# imported as part of fastai\n",
    "\n",
    "procs = [Categorify, FillMissing, Normalize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7523ae63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data randomly, adjust according to your needs\n",
    "splits = RandomSplitter(valid_pct=0.2)(range_of(pdf)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a0c48e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 - build the model\n",
    "\n",
    "to = TabularPandas(pdf, procs, cat_names, cont_names, y_names=dep_var, splits=splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "281a26ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# \n",
    "\n",
    "# Choose a batch size that fits your memory\n",
    "\n",
    "dls = to.dataloaders(bs=64)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b383b8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = tabular_learner(dls, layers=[200,100], metrics=accuracy)\n",
    "learn.fit_one_cycle(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e1c3f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be224995",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ab716f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c86ee56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87735b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the relevant libraries tocreate a pipleline\n",
    "\n",
    "#from pyspark.ml import Pipeline\n",
    "#from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StringIndexer, CountVectorizer, NGram, VectorAssembler, ChiSqSelector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aed335a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tokenizer \n",
    "\n",
    "#tokenizer = Tokenizer(inputCol=\"processed_tweet\", outputCol=\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965b456b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HashingTF: Hashing Term Frequency\n",
    "# REFERENCE - https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.HashingTF.html#pyspark.ml.feature.HashingTF\n",
    "# Maps a sequence of terms to their term frequencies using the hashing\n",
    "\n",
    "#hashtf = HashingTF(inputCol=\"words\", outputCol='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072f8d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the hashtf function to the IDF function\n",
    "# REFERENCE - https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.IDF.html#pyspark.ml.feature.IDF\n",
    "# Compute the Inverse Document Frequency (IDF) given a collection of documents ie the tweets df\n",
    "\n",
    "#idf = IDF(inputCol='tf', outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e798dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index labels\n",
    "\n",
    "#label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fdc4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "#lr = LogisticRegression()\n",
    "\n",
    "#pipeline = Pipeline(stages=[tokenizer, hashtf, idf, label_stringIdx, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9cd3e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6920f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
