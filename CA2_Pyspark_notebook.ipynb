{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ae21380",
   "metadata": {},
   "source": [
    "## CA 2 - Big Data & Advanced Analytics\n",
    "\n",
    "### Using Pyspark for data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "baea30d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear the cache on the spark session\n",
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b8b823f1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what version of pyspark is running on the computer using SparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f4ce1fe0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local[*]'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sc master - running locally\n",
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "16ff8a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import regex module\n",
    "import re\n",
    "from operator import add\n",
    "\n",
    "# Import Pyspark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Other Libraries \n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81f720fb",
   "metadata": {},
   "source": [
    "#### Import File from the Hadoop Directory\n",
    "\n",
    "<b>References</b>\n",
    "1. https://sparkbyexamples.com/pyspark/pyspark-read-csv-file-into-dataframe/\n",
    "2. https://medium.com/@ashutoshkumar2048/spark-connect-apache-spark-3-4-9846c40484d0\n",
    "3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "853ba5aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the spark session for CA2\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ca2\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5269d216",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a structure to hold the data, name and define data types\n",
    "schema = StructType([\n",
    "    StructField(\"target\", IntegerType(), True),\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"query\", StringType(), True),\n",
    "    StructField(\"author\", StringType(), True),\n",
    "    StructField(\"tweet\", StringType(), True)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d86ac3c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read input file from hadoop directory on the local drive\n",
    "# import the csv file from hadoop\n",
    "path = \"/user1/twitter_DS_1yr.csv\"\n",
    "\n",
    "df = spark.read.csv(path, header=False, inferSchema=True, schema=schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6cc39a5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import check - show the first 10 rows of the imported table\n",
    "\n",
    "#df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bbae2b9",
   "metadata": {},
   "source": [
    "#### Exploring the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8b95de54",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows:  1600000\n",
      "Number of columns:  6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Get the number of rows\n",
    "num_rows = df.count()\n",
    "\n",
    "# Get the number of columns\n",
    "num_columns = len(df.columns)\n",
    "\n",
    "# Print the shape\n",
    "print(\"Number of rows: \", num_rows)\n",
    "print(\"Number of columns: \", num_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c99b519",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 800000 positive values in the dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 800000 negative values in the dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# look at types of values in the polarity\n",
    "\n",
    "print(f\"There is {df[df['target']==4].count()} positive values in the dataframe.\")\n",
    "\n",
    "print(f\"There is {df[df['target']==0].count()} negative values in the dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b283edd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate entries\n",
    "\n",
    "df = df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4357a3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "3698fd2c",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffd47785",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libaries\n",
    "\n",
    "from pyspark.sql.functions import udf, regexp_replace, lower\n",
    "from pyspark.sql.types import StringType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "9e75a744",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[target: int, id: string, date: string, query: string, author: string, tweet: string]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop any n/a rows\n",
    "\n",
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "29e89e26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove noise such as html links, stop words / punctuation / #hashtags etc\n",
    "# REFERENCE: https://medium.com/towards-artificial-intelligence/large-scale-sentiment-analysis-with-pyspark-bdccf9256e35\n",
    "\n",
    "def pre_process(text):\n",
    "    # Remove links\n",
    "    #text = re.sub('http://\\S+|https://\\S+', '', text)\n",
    "    #text = re.sub('http[s]?://\\S+', '', text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "\n",
    "    # Convert HTML references\n",
    "    text = re.sub(r'&amp', 'and', text)\n",
    "    text = re.sub(r'&lt', '<', text)\n",
    "    text = re.sub(r'&gt', '>', text)\n",
    "    #text = re.sub(' ', text)\n",
    "\n",
    "    # Remove new line characters\n",
    "    text = re.sub(r'[\\r\\n]+', ' ', text)\n",
    "    \n",
    "    # Remove mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # Remove hashtags\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "\n",
    "    # Remove multiple space characters\n",
    "    text = re.sub(r'\\s+',' ', text)\n",
    "    \n",
    "    # Convert all text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "15a9d203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the pre_process function as a UDF (User-Defined Function)\n",
    "\n",
    "pre_process_udf = udf(pre_process, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0f39b1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the UDF to the 'Tweet_details' column and create a new column 'Processed_tweet_details'\n",
    "\n",
    "df = df.withColumn('processed_tweet', pre_process_udf('tweet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0415dc5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate entries\n",
    "\n",
    "df = df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5edaeeb2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ba741d85",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+--------+---------------+--------------------+--------------------+\n",
      "|target|        id|                date|   query|         author|               tweet|     processed_tweet|\n",
      "+------+----------+--------------------+--------+---------------+--------------------+--------------------+\n",
      "|     0|1468106999|Mon Apr 06 23:45:...|NO_QUERY|    stuartheron|@watko Shockingly...|    shockingly not! |\n",
      "|     0|1686953810|Sun May 03 07:19:...|NO_QUERY|forevaguitargrl|up ad ready for c...|up ad ready for c...|\n",
      "|     0|1956161267|Thu May 28 21:18:...|NO_QUERY|      SupaSista|Someone just sent...|someone just sent...|\n",
      "|     0|1956910895|Thu May 28 23:00:...|NO_QUERY| jeanettiewuvsu|Wtf is wrong with...|wtf is wrong with...|\n",
      "|     0|1963373312|Fri May 29 12:37:...|NO_QUERY|        XkyRauh|Only two weeks le...|only two weeks le...|\n",
      "|     0|1972460907|Sat May 30 09:16:...|NO_QUERY|         jiaaaa|You gave me false...|you gave me false...|\n",
      "|     0|1973029187|Sat May 30 10:20:...|NO_QUERY|   pinkkstarzzz|Still haven't got...|still haven't got...|\n",
      "|     0|1973874319|Sat May 30 12:00:...|NO_QUERY|       hopealot|@thejasonrobison ...| have fun!! i mis...|\n",
      "|     0|1974232831|Sat May 30 12:42:...|NO_QUERY|   christine426|I am craving crio...|i am craving crio...|\n",
      "|     0|1979507441|Sun May 31 03:20:...|NO_QUERY|       IRISMORE|@XChadballX tonig...| tonight , is new...|\n",
      "+------+----------+--------------------+--------+---------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Show the DataFrame with the new column\n",
    "\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a21abf5",
   "metadata": {},
   "source": [
    "#### Feature Extraction\n",
    "\n",
    "<b>References</b>\n",
    "1. https://medium.com/towards-artificial-intelligence/large-scale-sentiment-analysis-with-pyspark-bdccf9256e35\n",
    "\n",
    "2. https://medium.com/@chris_42047/an-easy-guide-to-basic-twitter-sentiment-analysis-python-tutorial-1630d5213ff6\n",
    "\n",
    "3. https://www.kaggle.com/code/muhammetzahitaydn/pyspark-sentiment-analysis-with-word2vec-embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "96712e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the relevant libraries tocreate a pipleline\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StringIndexer, CountVectorizer, NGram, VectorAssembler, ChiSqSelector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "9a3c160f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tokenizer \n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"processed_tweet\", outputCol=\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ac2a4907",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HashingTF: Hashing Term Frequency\n",
    "# REFERENCE - https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.HashingTF.html#pyspark.ml.feature.HashingTF\n",
    "# Maps a sequence of terms to their term frequencies using the hashing\n",
    "\n",
    "hashtf = HashingTF(inputCol=\"words\", outputCol='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c09199a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the hashtf function to the IDF function\n",
    "# REFERENCE - https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.IDF.html#pyspark.ml.feature.IDF\n",
    "# Compute the Inverse Document Frequency (IDF) given a collection of documents ie the tweets df\n",
    "\n",
    "idf = IDF(inputCol='tf', outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e10785f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index labels\n",
    "\n",
    "label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6ff66134",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, hashtf, idf, label_stringIdx, lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60630ff3",
   "metadata": {},
   "source": [
    "#### Filter data based on key word - 'weather'\n",
    "\n",
    "##### REFERENCES \n",
    "1. https://towardsdatascience.com/sentiment-analysis-with-pyspark-bc8e83f80c35\n",
    "2. https://towardsdatascience.com/my-absolute-go-to-for-sentiment-analysis-textblob-3ac3a11d524"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81cc0d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a variable to search within the dataframe on\n",
    "\n",
    "search_for = \"weather\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e5bab3ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.filter(df[\"processed_tweet\"].contains(search_for))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b6822572",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:=====================================================>(199 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+--------+---------------+--------------------+--------------------+\n",
      "|target|        id|                date|   query|         author|               tweet|     processed_tweet|\n",
      "+------+----------+--------------------+--------+---------------+--------------------+--------------------+\n",
      "|     4|1964587188|Fri May 29 14:28:...|NO_QUERY|       SuperRob|@sparktography We...| weather.com says...|\n",
      "|     0|2264778973|Sun Jun 21 04:55:...|NO_QUERY|    deirdre7194|@Es94 yay!! i did...| yay!! i didnt no...|\n",
      "|     4|2060752623|Sat Jun 06 19:30:...|NO_QUERY|     HailHorror|Finishing my pack...|finishing my pack...|\n",
      "|     0|2229093418|Thu Jun 18 15:16:...|NO_QUERY| nocheapthrillz|@afreshmusic hey ...| hey d! my bad, i...|\n",
      "|     0|2191448262|Tue Jun 16 05:17:...|NO_QUERY|     tippielove|PS for those intr...|ps for those intr...|\n",
      "|     0|2222121192|Thu Jun 18 06:31:...|NO_QUERY|  abitheamazing|so its nice weath...|so its nice weath...|\n",
      "|     0|1992186304|Mon Jun 01 08:20:...|NO_QUERY|     CalmedFury|Why did the weath...|why did the weath...|\n",
      "|     4|2001516219|Tue Jun 02 01:08:...|NO_QUERY|       Renatuuh|I just love long ...|i just love long ...|\n",
      "|     4|1988818733|Sun May 31 23:27:...|NO_QUERY|stroughtonsmith|@MadMax you'd fin...| you'd find other...|\n",
      "|     4|1793480642|Thu May 14 02:51:...|NO_QUERY|      winterdew|Well, it will hit...|well, it will hit...|\n",
      "+------+----------+--------------------+--------+---------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "72d66b0c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Get the number of rows in the new df\n",
    "num_rows = df2.count()\n",
    "\n",
    "# Get the number of columns\n",
    "num_columns = len(df2.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "5b058cdf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows:  10342\n",
      "Number of columns:  7\n"
     ]
    }
   ],
   "source": [
    "# Print the shape\n",
    "print(\"Number of rows: \", num_rows)\n",
    "print(\"Number of columns: \", num_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80441449",
   "metadata": {},
   "source": [
    "### **************************************************************\n",
    "\n",
    "\n",
    "\n",
    "#### Sentiment extraction using TextBlob\n",
    "\n",
    "##### REFERENCES\n",
    "\n",
    "1. https://towardsdatascience.com/my-absolute-go-to-for-sentiment-analysis-textblob-3ac3a11d524\n",
    "2. https://towardsdatascience.com/sentiment-analysis-with-pyspark-bc8e83f80c35\n",
    "3. https://towardsdatascience.com/my-absolute-go-to-for-sentiment-analysis-textblob-3ac3a11d524"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "dce68307",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from textblob import TextBlob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d15a73fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a UDF to apply sentiment analysis using TextBlob\n",
    "\n",
    "def get_sentiment(processed_tweet):\n",
    "    analysis = TextBlob(processed_tweet)\n",
    "    \n",
    "    sentiment = analysis.sentiment.polarity\n",
    "\n",
    "    return sentiment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1cd93a56",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Register the UDF\n",
    "get_sentiment_udf = spark.udf.register(\"get_sentiment\", get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "af564edf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the UDF to the 'tweet' column and create a new column 'sentiment'\n",
    "df = df.withColumn('sentiment', get_sentiment_udf('processed_tweet'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "3ee9a853",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+\n",
      "|processed_tweet                                                                                                                                    |sentiment          |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+\n",
      "| shockingly not!                                                                                                                                   |0.5                |\n",
      "|up ad ready for church, gotta start packing today... really starting to think about how much i'm gonna miss ethan this summer.                     |0.13333333333333333|\n",
      "|someone just sent a text message, but i can't see it.                                                                                              |0.0                |\n",
      "|wtf is wrong with me..im not like this....i need some sleep.                                                                                       |-0.5               |\n",
      "|only two weeks left at my middle school gig. full-time work over the summer, but teaching credential begins at the end of august!                  |0.0                |\n",
      "|you gave me false hope                                                                                                                             |-0.4000000000000001|\n",
      "|still haven't gotten rid of this damn cold!!! and denver lost last night! grrrr                                                                    |-0.5666666666666667|\n",
      "| have fun!! i miss getting to hang out on saturdays!                                                                                               |0.5859375          |\n",
      "|i am craving criolla empanadas... but settling for mcdonalds instead. so not the same                                                              |0.0                |\n",
      "| tonight , is new found glory coming to belgium, and i can't be there , omgosh                                                                     |0.13636363636363635|\n",
      "|family won't get home till after midnight... i was really looking forward to seeing them... within the hour lonely.                                |0.05000000000000002|\n",
      "| i think they are all fake                                                                                                                         |-0.5               |\n",
      "| but anywhooo that sandwich usually cost $11 alone!! and it was $5...i think my whole life just changed and took a turn towards                    |-0.0953125         |\n",
      "| kodos lives with marc. lorne is a better guard, though. kodos would just lick a burglar's hand. ;)                                                |0.375              |\n",
      "|about to get my fast fourier on for last exam - computer vision                                                                                    |0.1                |\n",
      "|a cigarette yep.. just note i need to go and buy some.... i hate life!                                                                             |-1.0               |\n",
      "|our great lil' dog of 16 yrs, &quot;max&quot; died this past wknd. i couldn't work in studio today w/out him nearby as always~ took laptop 2 lake! |0.24375000000000002|\n",
      "| argh shit! i wish i lived in the uk                                                                                                               |-0.25              |\n",
      "|on my way home from work...aint no telling what i will encounter on the cta...                                                                     |0.0                |\n",
      "|what do i do now that basketball season's over... no more everywhere i turn.. makes me sad...                                                      |-0.375             |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Show the DataFrame with the 'tweet' and 'sentiment' columns\n",
    "df.select('processed_tweet', 'sentiment').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181dc9de",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "34346aaf",
   "metadata": {},
   "source": [
    "#### Save Weather Data to a new Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "17dc0f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REFERENCE - https://sparkbyexamples.com/pyspark/pyspark-write-dataframe-to-csv-file/\n",
    "\n",
    "#rename the dataframe\n",
    "\n",
    "weather_tweets2 = df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a05685e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the file to a csv file\n",
    "\n",
    "#weather_tweets.write.csv(\"hdfs://localhost:9000/user1/weather_tweets2\")\n",
    "\n",
    "# commented out as file already exists on Hadop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5062fb1e",
   "metadata": {},
   "source": [
    "### Time Series\n",
    "\n",
    "##### References\n",
    "\n",
    "1. https://towardsdatascience.com/end-to-end-time-series-interpolation-in-pyspark-filling-the-gap-5ccefc6b7fc9\n",
    "2. https://medium.com/@y.s.yoon/scalable-time-series-forecasting-in-spark-prophet-cnn-lstm-and-sarima-a5306153711e\n",
    "3. https://medium.com/delaware-pro/interpolate-big-data-time-series-in-native-pyspark-d270d4b592a1\n",
    "4. https://www.fast.ai/    and   https://github.com/fastai\n",
    "6. https://docs.fast.ai/tutorial.text.html\n",
    "\n",
    "****\n",
    "Fastai is a deep learning library that was used to complete the analsysis.  This was the selected algorithm as it can be used with pyspark.  Finally the library can work with tabular and text data so can be used to complete a time series and a sentiment analysis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "89272d77",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Import Libraries \n",
    "\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from fastai.tabular import *\n",
    "import six\n",
    "\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DateType, IntegerType, FloatType\n",
    "\n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "69b0c211",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put the data in chronological order by date\n",
    "\n",
    "weather_tweets2 = weather_tweets2.orderBy('date')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "c0c8765d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# convert weather_tweets 2 from a spark df to a Pandas df for processing\n",
    "pdf = weather_tweets2.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "769189f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 10342 entries, 0 to 10341\n",
      "Data columns (total 7 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   target           10342 non-null  int32 \n",
      " 1   id               10342 non-null  object\n",
      " 2   date             10342 non-null  object\n",
      " 3   query            10342 non-null  object\n",
      " 4   author           10342 non-null  object\n",
      " 5   tweet            10342 non-null  object\n",
      " 6   processed_tweet  10342 non-null  object\n",
      "dtypes: int32(1), object(6)\n",
      "memory usage: 525.3+ KB\n"
     ]
    }
   ],
   "source": [
    "# display the resulting pandas dataframe\n",
    "\n",
    "pdf.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60dd2267",
   "metadata": {},
   "source": [
    "#### Preparing for the ML Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "ddeb47b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare to use fastai library to complete the analysis \n",
    "# Step 1 - defining variables fpr the analysis\n",
    "# REFERNCE - https://docs.fast.ai/quick_start.html\n",
    "\n",
    "\n",
    "target_var = 'processed_tweet'\n",
    "categorical_vars = ['target', 'author']\n",
    "date_var = ['date']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2925d91a",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'Categorify' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17742/1161134227.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprocs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mCategorify\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFillMissing\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mNormalize\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'Categorify' is not defined"
     ]
    }
   ],
   "source": [
    "# a list of preprocessing operations used on the data for training\n",
    "# imported as part of fastai\n",
    "\n",
    "procs = [Categorify, FillMissing, Normalize]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7523ae63",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TabularList' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17742/561994645.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# REFERNCE - https://docs.fast.ai/quick_start.html\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m data_list = (TabularList.from_df(pdf, \n\u001b[0m\u001b[1;32m      6\u001b[0m                                  target_var=target_var, categorical_vars=categorical_vars, procs=[FillMissing, Categorify, Normalize])\n\u001b[1;32m      7\u001b[0m                    \u001b[0;34m.\u001b[0m\u001b[0msplit_by_idx\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m800\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# split data between 800 & 1000 for data validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'TabularList' is not defined"
     ]
    }
   ],
   "source": [
    "# Split data randomly, adjust according to your needs\n",
    "splits = RandomSplitter(valid_pct=0.2)(range_of(pdf)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "03a0c48e",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'TabularPandas' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17742/3347160176.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Step 2 - build the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mto\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTabularPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpdf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprocs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcont_names\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdep_var\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msplits\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msplits\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'TabularPandas' is not defined"
     ]
    }
   ],
   "source": [
    "# Step 2 - build the model\n",
    "\n",
    "to = TabularPandas(pdf, procs, cat_names, cont_names, y_names=dep_var, splits=splits)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "281a26ab",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'to' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17742/24733861.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m# Choose a batch size that fits your memory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdls\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mto\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataloaders\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m64\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'to' is not defined"
     ]
    }
   ],
   "source": [
    "# \n",
    "\n",
    "# Choose a batch size that fits your memory\n",
    "\n",
    "dls = to.dataloaders(bs=64)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b383b8fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "learn = tabular_learner(dls, layers=[200,100], metrics=accuracy)\n",
    "learn.fit_one_cycle(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2e1c3f2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be224995",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59ab716f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c86ee56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87735b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the relevant libraries tocreate a pipleline\n",
    "\n",
    "#from pyspark.ml import Pipeline\n",
    "#from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StringIndexer, CountVectorizer, NGram, VectorAssembler, ChiSqSelector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6aed335a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tokenizer \n",
    "\n",
    "#tokenizer = Tokenizer(inputCol=\"processed_tweet\", outputCol=\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965b456b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HashingTF: Hashing Term Frequency\n",
    "# REFERENCE - https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.HashingTF.html#pyspark.ml.feature.HashingTF\n",
    "# Maps a sequence of terms to their term frequencies using the hashing\n",
    "\n",
    "#hashtf = HashingTF(inputCol=\"words\", outputCol='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "072f8d3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the hashtf function to the IDF function\n",
    "# REFERENCE - https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.IDF.html#pyspark.ml.feature.IDF\n",
    "# Compute the Inverse Document Frequency (IDF) given a collection of documents ie the tweets df\n",
    "\n",
    "#idf = IDF(inputCol='tf', outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e798dff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index labels\n",
    "\n",
    "#label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fdc4fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "#lr = LogisticRegression()\n",
    "\n",
    "#pipeline = Pipeline(stages=[tokenizer, hashtf, idf, label_stringIdx, lr])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a9cd3e4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a6920f1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
