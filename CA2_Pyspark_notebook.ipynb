{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab30b85f",
   "metadata": {},
   "source": [
    "## CA 2 - Big Data & Advanced Analytics\n",
    "\n",
    "### Using Pyspark for data exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fe9124f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clear the cache on the spark session\n",
    "spark.catalog.clearCache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df8335f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://10.0.2.15:4041\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.1.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>PySparkShell</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        "
      ],
      "text/plain": [
       "<SparkContext master=local[*] appName=PySparkShell>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# what version of pyspark is running on the computer using SparkContext\n",
    "sc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dae3c839",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'local[*]'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sc master - running locally\n",
    "sc.master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bdeb267b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import regex module\n",
    "import re\n",
    "from operator import add\n",
    "\n",
    "# Import Pyspark\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Other Libraries \n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "\n",
    "from pyspark.ml.feature import MinMaxScaler\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac61769",
   "metadata": {},
   "source": [
    "#### Import File from the Hadoop Directory\n",
    "\n",
    "<b>References</b>\n",
    "1. https://sparkbyexamples.com/pyspark/pyspark-read-csv-file-into-dataframe/\n",
    "2. https://medium.com/@ashutoshkumar2048/spark-connect-apache-spark-3-4-9846c40484d0\n",
    "3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1c4fab2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# start the spark session for CA2\n",
    "\n",
    "spark = SparkSession.builder.appName(\"ca2\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92b1efc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a structure to hold the data, name and define data types\n",
    "schema = StructType([\n",
    "    StructField(\"target\", IntegerType(), True),\n",
    "    StructField(\"id\", StringType(), True),\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"query\", StringType(), True),\n",
    "    StructField(\"author\", StringType(), True),\n",
    "    StructField(\"tweet\", StringType(), True)])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "745a719d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read input file from hadoop directory on the local drive\n",
    "# import the csv file from hadoop\n",
    "path = \"/user1/twitter_DS_1yr.csv\"\n",
    "\n",
    "df = spark.read.csv(path, header=False, inferSchema=True, schema=schema)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "35bcd804",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import check - show the first 10 rows of the imported table\n",
    "\n",
    "#df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1370098d",
   "metadata": {},
   "source": [
    "#### Exploring the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3850bac",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows:  1600000\n",
      "Number of columns:  6\n"
     ]
    }
   ],
   "source": [
    "# Get the number of rows\n",
    "num_rows = df.count()\n",
    "\n",
    "# Get the number of columns\n",
    "num_columns = len(df.columns)\n",
    "\n",
    "# Print the shape\n",
    "print(\"Number of rows: \", num_rows)\n",
    "print(\"Number of columns: \", num_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7ee9a971",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 800000 positive values in the dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 4:=============================>                             (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is 800000 negative values in the dataframe.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# look at types of values in the polarity\n",
    "\n",
    "print(f\"There is {df[df['target']==4].count()} positive values in the dataframe.\")\n",
    "\n",
    "print(f\"There is {df[df['target']==0].count()} negative values in the dataframe.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ce0554d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate entries\n",
    "\n",
    "df = df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f9b2da",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "b9373933",
   "metadata": {},
   "source": [
    "#### Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ed475484",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import relevant libaries\n",
    "\n",
    "from pyspark.sql.functions import udf, regexp_replace, lower\n",
    "from pyspark.sql.types import StringType\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de4ee8c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DataFrame[target: int, id: string, date: string, query: string, author: string, tweet: string]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# drop any n/a rows\n",
    "\n",
    "df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "228cddfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove noise such as html links, stop words / punctuation / #hashtags etc\n",
    "# REFERENCE: https://medium.com/towards-artificial-intelligence/large-scale-sentiment-analysis-with-pyspark-bdccf9256e35\n",
    "\n",
    "def pre_process(text):\n",
    "    # Remove links\n",
    "    #text = re.sub('http://\\S+|https://\\S+', '', text)\n",
    "    #text = re.sub('http[s]?://\\S+', '', text)\n",
    "    text = re.sub(r'http\\S+', '', text)\n",
    "\n",
    "    # Convert HTML references\n",
    "    text = re.sub(r'&amp', 'and', text)\n",
    "    text = re.sub(r'&lt', '<', text)\n",
    "    text = re.sub(r'&gt', '>', text)\n",
    "    #text = re.sub(' ', text)\n",
    "\n",
    "    # Remove new line characters\n",
    "    text = re.sub(r'[\\r\\n]+', ' ', text)\n",
    "    \n",
    "    # Remove mentions\n",
    "    text = re.sub(r'@\\w+', '', text)\n",
    "    \n",
    "    # Remove hashtags\n",
    "    text = re.sub(r'#\\w+', '', text)\n",
    "\n",
    "    # Remove multiple space characters\n",
    "    text = re.sub(r'\\s+',' ', text)\n",
    "    \n",
    "    # Convert all text to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d0a714e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register the pre_process function as a UDF (User-Defined Function)\n",
    "\n",
    "pre_process_udf = udf(pre_process, StringType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8903dea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the UDF to the 'Tweet_details' column and create a new column 'Processed_tweet_details'\n",
    "\n",
    "df = df.withColumn('processed_tweet', pre_process_udf('tweet'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "85134e3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop duplicate entries\n",
    "\n",
    "df = df.dropDuplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a331e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "719cb7c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 7:======================================================>(199 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+--------+---------------+--------------------+--------------------+\n",
      "|target|        id|                date|   query|         author|               tweet|     processed_tweet|\n",
      "+------+----------+--------------------+--------+---------------+--------------------+--------------------+\n",
      "|     0|1468106999|Mon Apr 06 23:45:...|NO_QUERY|    stuartheron|@watko Shockingly...|    shockingly not! |\n",
      "|     0|1686953810|Sun May 03 07:19:...|NO_QUERY|forevaguitargrl|up ad ready for c...|up ad ready for c...|\n",
      "|     0|1956161267|Thu May 28 21:18:...|NO_QUERY|      SupaSista|Someone just sent...|someone just sent...|\n",
      "|     0|1956910895|Thu May 28 23:00:...|NO_QUERY| jeanettiewuvsu|Wtf is wrong with...|wtf is wrong with...|\n",
      "|     0|1963373312|Fri May 29 12:37:...|NO_QUERY|        XkyRauh|Only two weeks le...|only two weeks le...|\n",
      "|     0|1972460907|Sat May 30 09:16:...|NO_QUERY|         jiaaaa|You gave me false...|you gave me false...|\n",
      "|     0|1973029187|Sat May 30 10:20:...|NO_QUERY|   pinkkstarzzz|Still haven't got...|still haven't got...|\n",
      "|     0|1973874319|Sat May 30 12:00:...|NO_QUERY|       hopealot|@thejasonrobison ...| have fun!! i mis...|\n",
      "|     0|1974232831|Sat May 30 12:42:...|NO_QUERY|   christine426|I am craving crio...|i am craving crio...|\n",
      "|     0|1979507441|Sun May 31 03:20:...|NO_QUERY|       IRISMORE|@XChadballX tonig...| tonight , is new...|\n",
      "+------+----------+--------------------+--------+---------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Show the DataFrame with the new column\n",
    "\n",
    "df.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6385f0da",
   "metadata": {},
   "source": [
    "#### Feature Extraction\n",
    "\n",
    "<b>References</b>\n",
    "1. https://medium.com/towards-artificial-intelligence/large-scale-sentiment-analysis-with-pyspark-bdccf9256e35\n",
    "\n",
    "2. https://medium.com/@chris_42047/an-easy-guide-to-basic-twitter-sentiment-analysis-python-tutorial-1630d5213ff6\n",
    "\n",
    "3. https://www.kaggle.com/code/muhammetzahitaydn/pyspark-sentiment-analysis-with-word2vec-embedding\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "483d5a62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the relevant libraries tocreate a piplelin\n",
    "\n",
    "from pyspark.ml import Pipeline\n",
    "from pyspark.ml.feature import HashingTF, IDF, Tokenizer, StringIndexer, CountVectorizer, NGram, VectorAssembler, ChiSqSelector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3c60136a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a tokenizer \n",
    "\n",
    "tokenizer = Tokenizer(inputCol=\"processed_tweet\", outputCol=\"words\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e42f7904",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HashingTF: Hashing Term Frequency\n",
    "# REFERENCE - https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.HashingTF.html#pyspark.ml.feature.HashingTF\n",
    "# Maps a sequence of terms to their term frequencies using the hashing\n",
    "\n",
    "hashtf = HashingTF(inputCol=\"words\", outputCol='tf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "924e4a76",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pass the hashtf function to the IDF function\n",
    "# REFERENCE - https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.ml.feature.IDF.html#pyspark.ml.feature.IDF\n",
    "# Compute the Inverse Document Frequency (IDF) given a collection of documents ie the tweets df\n",
    "\n",
    "idf = IDF(inputCol='tf', outputCol=\"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "84e12e42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Index labels\n",
    "\n",
    "label_stringIdx = StringIndexer(inputCol = \"target\", outputCol = \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "9618d234",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegression\n",
    "\n",
    "lr = LogisticRegression()\n",
    "\n",
    "pipeline = Pipeline(stages=[tokenizer, hashtf, idf, label_stringIdx, lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e616f59",
   "metadata": {},
   "source": [
    "#### Filter data based on key word - 'weather'\n",
    "\n",
    "##### REFERENCES \n",
    "1. https://towardsdatascience.com/sentiment-analysis-with-pyspark-bc8e83f80c35\n",
    "2. https://towardsdatascience.com/my-absolute-go-to-for-sentiment-analysis-textblob-3ac3a11d524"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fd5a4d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a variable to search within the dataframe on\n",
    "\n",
    "search_for = \"weather\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d8bd99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.filter(df[\"processed_tweet\"].contains(search_for))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "049a984e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:=====================================================>(198 + 1) / 200]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----------+--------------------+--------+---------------+--------------------+--------------------+\n",
      "|target|        id|                date|   query|         author|               tweet|     processed_tweet|\n",
      "+------+----------+--------------------+--------+---------------+--------------------+--------------------+\n",
      "|     4|1964587188|Fri May 29 14:28:...|NO_QUERY|       SuperRob|@sparktography We...| weather.com says...|\n",
      "|     0|2264778973|Sun Jun 21 04:55:...|NO_QUERY|    deirdre7194|@Es94 yay!! i did...| yay!! i didnt no...|\n",
      "|     4|2060752623|Sat Jun 06 19:30:...|NO_QUERY|     HailHorror|Finishing my pack...|finishing my pack...|\n",
      "|     0|2229093418|Thu Jun 18 15:16:...|NO_QUERY| nocheapthrillz|@afreshmusic hey ...| hey d! my bad, i...|\n",
      "|     0|2191448262|Tue Jun 16 05:17:...|NO_QUERY|     tippielove|PS for those intr...|ps for those intr...|\n",
      "|     0|2222121192|Thu Jun 18 06:31:...|NO_QUERY|  abitheamazing|so its nice weath...|so its nice weath...|\n",
      "|     0|1992186304|Mon Jun 01 08:20:...|NO_QUERY|     CalmedFury|Why did the weath...|why did the weath...|\n",
      "|     4|2001516219|Tue Jun 02 01:08:...|NO_QUERY|       Renatuuh|I just love long ...|i just love long ...|\n",
      "|     4|1988818733|Sun May 31 23:27:...|NO_QUERY|stroughtonsmith|@MadMax you'd fin...| you'd find other...|\n",
      "|     4|1793480642|Thu May 14 02:51:...|NO_QUERY|      winterdew|Well, it will hit...|well, it will hit...|\n",
      "+------+----------+--------------------+--------+---------------+--------------------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 10:=====================================================>(199 + 1) / 200]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df2.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8d15624d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Get the number of rows in the new df\n",
    "num_rows = df2.count()\n",
    "\n",
    "# Get the number of columns\n",
    "num_columns = len(df2.columns)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7d780f44",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows:  10342\n",
      "Number of columns:  7\n"
     ]
    }
   ],
   "source": [
    "# Print the shape\n",
    "print(\"Number of rows: \", num_rows)\n",
    "print(\"Number of columns: \", num_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75622b72",
   "metadata": {},
   "source": [
    "### **************************************************************\n",
    "\n",
    "\n",
    "\n",
    "#### Sentiment extraction using TextBlob\n",
    "\n",
    "##### REFERENCES\n",
    "\n",
    "1. https://towardsdatascience.com/my-absolute-go-to-for-sentiment-analysis-textblob-3ac3a11d524\n",
    "2. https://towardsdatascience.com/sentiment-analysis-with-pyspark-bc8e83f80c35\n",
    "3. https://towardsdatascience.com/my-absolute-go-to-for-sentiment-analysis-textblob-3ac3a11d524"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "f01840e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from textblob import TextBlob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "81d904c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a UDF to apply sentiment analysis using TextBlob\n",
    "\n",
    "def get_sentiment(processed_tweet):\n",
    "    analysis = TextBlob(processed_tweet)\n",
    "    \n",
    "    sentiment = analysis.sentiment.polarity\n",
    "\n",
    "    return sentiment\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7b1a6818",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Register the UDF\n",
    "get_sentiment_udf = spark.udf.register(\"get_sentiment\", get_sentiment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "3271b45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply the UDF to the 'tweet' column and create a new column 'sentiment'\n",
    "df = df.withColumn('sentiment', get_sentiment_udf('processed_tweet'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c2c62fc7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 18:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+\n",
      "|processed_tweet                                                                                                                                    |sentiment          |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+\n",
      "| shockingly not!                                                                                                                                   |0.5                |\n",
      "|up ad ready for church, gotta start packing today... really starting to think about how much i'm gonna miss ethan this summer.                     |0.13333333333333333|\n",
      "|someone just sent a text message, but i can't see it.                                                                                              |0.0                |\n",
      "|wtf is wrong with me..im not like this....i need some sleep.                                                                                       |-0.5               |\n",
      "|only two weeks left at my middle school gig. full-time work over the summer, but teaching credential begins at the end of august!                  |0.0                |\n",
      "|you gave me false hope                                                                                                                             |-0.4000000000000001|\n",
      "|still haven't gotten rid of this damn cold!!! and denver lost last night! grrrr                                                                    |-0.5666666666666667|\n",
      "| have fun!! i miss getting to hang out on saturdays!                                                                                               |0.5859375          |\n",
      "|i am craving criolla empanadas... but settling for mcdonalds instead. so not the same                                                              |0.0                |\n",
      "| tonight , is new found glory coming to belgium, and i can't be there , omgosh                                                                     |0.13636363636363635|\n",
      "|family won't get home till after midnight... i was really looking forward to seeing them... within the hour lonely.                                |0.05000000000000002|\n",
      "| i think they are all fake                                                                                                                         |-0.5               |\n",
      "| but anywhooo that sandwich usually cost $11 alone!! and it was $5...i think my whole life just changed and took a turn towards                    |-0.0953125         |\n",
      "| kodos lives with marc. lorne is a better guard, though. kodos would just lick a burglar's hand. ;)                                                |0.375              |\n",
      "|about to get my fast fourier on for last exam - computer vision                                                                                    |0.1                |\n",
      "|a cigarette yep.. just note i need to go and buy some.... i hate life!                                                                             |-1.0               |\n",
      "|our great lil' dog of 16 yrs, &quot;max&quot; died this past wknd. i couldn't work in studio today w/out him nearby as always~ took laptop 2 lake! |0.24375000000000002|\n",
      "| argh shit! i wish i lived in the uk                                                                                                               |-0.25              |\n",
      "|on my way home from work...aint no telling what i will encounter on the cta...                                                                     |0.0                |\n",
      "|what do i do now that basketball season's over... no more everywhere i turn.. makes me sad...                                                      |-0.375             |\n",
      "+---------------------------------------------------------------------------------------------------------------------------------------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "# Show the DataFrame with the 'tweet' and 'sentiment' columns\n",
    "df.select('processed_tweet', 'sentiment').show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455fed0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c0f5a377",
   "metadata": {},
   "source": [
    "#### Save Weather Data to a new Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "93d04672",
   "metadata": {},
   "outputs": [],
   "source": [
    "# REFERENCE - https://sparkbyexamples.com/pyspark/pyspark-write-dataframe-to-csv-file/\n",
    "\n",
    "#rename the dataframe\n",
    "\n",
    "weather_tweets2 = df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8e6ae420",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-21 17:54:52,866 WARN hdfs.DataStreamer: Caught exception  (2 + 1) / 200]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeInternal(DataStreamer.java:847)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:843)\n",
      "2023-05-21 17:54:53,135 WARN hdfs.DataStreamer: Caught exception  (3 + 1) / 200]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n",
      "2023-05-21 17:54:53,389 WARN hdfs.DataStreamer: Caught exception  (4 + 1) / 200]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n",
      "2023-05-21 17:54:54,382 WARN hdfs.DataStreamer: Caught exception  (9 + 1) / 200]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n",
      "2023-05-21 17:54:54,549 WARN hdfs.DataStreamer: Caught exception (11 + 1) / 200]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n",
      "2023-05-21 17:54:55,325 WARN hdfs.DataStreamer: Caught exception (15 + 1) / 200]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeInternal(DataStreamer.java:847)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:843)\n",
      "2023-05-21 17:54:55,524 WARN hdfs.DataStreamer: Caught exception (16 + 1) / 200]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n",
      "2023-05-21 17:54:56,603 WARN hdfs.DataStreamer: Caught exception (21 + 1) / 200]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n",
      "2023-05-21 17:54:57,109 WARN hdfs.DataStreamer: Caught exception (25 + 1) / 200]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeInternal(DataStreamer.java:847)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:843)\n",
      "2023-05-21 17:54:57,775 WARN hdfs.DataStreamer: Caught exception (28 + 1) / 200]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeInternal(DataStreamer.java:847)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:843)\n",
      "2023-05-21 17:54:57,965 WARN hdfs.DataStreamer: Caught exception (30 + 1) / 200]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeInternal(DataStreamer.java:847)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:843)\n",
      "2023-05-21 17:54:58,426 WARN hdfs.DataStreamer: Caught exception (32 + 1) / 200]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n",
      "2023-05-21 17:54:58,620 WARN hdfs.DataStreamer: Caught exception (34 + 1) / 200]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeInternal(DataStreamer.java:847)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:843)\n",
      "2023-05-21 17:54:58,720 WARN hdfs.DataStreamer: Caught exception (35 + 1) / 200]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeInternal(DataStreamer.java:847)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:843)\n",
      "2023-05-21 17:54:58,830 WARN hdfs.DataStreamer: Caught exception\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeInternal(DataStreamer.java:847)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:843)\n",
      "2023-05-21 17:55:00,453 WARN hdfs.DataStreamer: Caught exception (41 + 1) / 200]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n",
      "2023-05-21 17:55:00,728 WARN hdfs.DataStreamer: Caught exception (44 + 1) / 200]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeInternal(DataStreamer.java:847)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:843)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-21 17:55:02,156 WARN hdfs.DataStreamer: Caught exception (49 + 1) / 200]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n",
      "2023-05-21 17:55:02,309 WARN hdfs.DataStreamer: Caught exception (51 + 1) / 200]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeInternal(DataStreamer.java:847)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:843)\n",
      "2023-05-21 17:55:02,422 WARN hdfs.DataStreamer: Caught exception\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n",
      "2023-05-21 17:55:03,501 WARN hdfs.DataStreamer: Caught exception (57 + 1) / 200]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n",
      "2023-05-21 17:55:03,627 WARN hdfs.DataStreamer: Caught exception\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeInternal(DataStreamer.java:847)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:843)\n",
      "2023-05-21 17:55:03,753 WARN hdfs.DataStreamer: Caught exception (59 + 1) / 200]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeInternal(DataStreamer.java:847)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:843)\n",
      "2023-05-21 17:55:03,901 WARN hdfs.DataStreamer: Caught exception (60 + 1) / 200]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeInternal(DataStreamer.java:847)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:843)\n",
      "2023-05-21 17:55:04,322 WARN hdfs.DataStreamer: Caught exception (62 + 1) / 200]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeInternal(DataStreamer.java:847)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:843)\n",
      "2023-05-21 17:55:04,712 WARN hdfs.DataStreamer: Caught exception (65 + 1) / 200]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeInternal(DataStreamer.java:847)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:843)\n",
      "2023-05-21 17:55:04,846 WARN hdfs.DataStreamer: Caught exception\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeInternal(DataStreamer.java:847)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:843)\n",
      "2023-05-21 17:55:05,202 WARN hdfs.DataStreamer: Caught exception (68 + 1) / 200]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n",
      "2023-05-21 17:55:05,366 WARN hdfs.DataStreamer: Caught exception (69 + 1) / 200]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeInternal(DataStreamer.java:847)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:843)\n",
      "2023-05-21 17:55:05,653 WARN hdfs.DataStreamer: Caught exception (70 + 1) / 200]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeInternal(DataStreamer.java:847)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:843)\n",
      "2023-05-21 17:55:07,825 WARN hdfs.DataStreamer: Caught exception (89 + 1) / 200]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeInternal(DataStreamer.java:847)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:843)\n",
      "2023-05-21 17:55:08,887 WARN hdfs.DataStreamer: Caught exception (93 + 1) / 200]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n",
      "2023-05-21 17:55:10,445 WARN hdfs.DataStreamer: Caught exception(106 + 1) / 200]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeInternal(DataStreamer.java:847)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:843)\n",
      "2023-05-21 17:55:11,877 WARN hdfs.DataStreamer: Caught exception(112 + 1) / 200]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeInternal(DataStreamer.java:847)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:843)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-21 17:55:12,436 WARN hdfs.DataStreamer: Caught exception(115 + 1) / 200]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeInternal(DataStreamer.java:847)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:843)\n",
      "2023-05-21 17:55:12,544 WARN hdfs.DataStreamer: Caught exception\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n",
      "2023-05-21 17:55:12,797 WARN hdfs.DataStreamer: Caught exception(119 + 1) / 200]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeInternal(DataStreamer.java:847)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:843)\n",
      "2023-05-21 17:55:13,033 WARN hdfs.DataStreamer: Caught exception(121 + 1) / 200]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeInternal(DataStreamer.java:847)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:843)\n",
      "2023-05-21 17:55:13,138 WARN hdfs.DataStreamer: Caught exception\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n",
      "2023-05-21 17:55:13,795 WARN hdfs.DataStreamer: Caught exception(127 + 1) / 200]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n",
      "2023-05-21 17:55:13,937 WARN hdfs.DataStreamer: Caught exception\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n",
      "2023-05-21 17:55:15,035 WARN hdfs.DataStreamer: Caught exception(136 + 1) / 200]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n",
      "2023-05-21 17:55:15,330 WARN hdfs.DataStreamer: Caught exception(137 + 1) / 200]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeInternal(DataStreamer.java:847)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:843)\n",
      "2023-05-21 17:55:15,681 WARN hdfs.DataStreamer: Caught exception(140 + 1) / 200]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n",
      "2023-05-21 17:55:16,058 WARN hdfs.DataStreamer: Caught exception(143 + 1) / 200]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeInternal(DataStreamer.java:847)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:843)\n",
      "2023-05-21 17:55:16,799 WARN hdfs.DataStreamer: Caught exception(148 + 1) / 200]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n",
      "2023-05-21 17:55:17,103 WARN hdfs.DataStreamer: Caught exception(149 + 1) / 200]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n",
      "2023-05-21 17:55:17,237 WARN hdfs.DataStreamer: Caught exception(151 + 1) / 200]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeInternal(DataStreamer.java:847)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:843)\n",
      "2023-05-21 17:55:18,239 WARN hdfs.DataStreamer: Caught exception(159 + 1) / 200]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n",
      "2023-05-21 17:55:18,605 WARN hdfs.DataStreamer: Caught exception(163 + 1) / 200]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeInternal(DataStreamer.java:847)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:843)\n",
      "2023-05-21 17:55:19,822 WARN hdfs.DataStreamer: Caught exception(170 + 1) / 200]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeInternal(DataStreamer.java:847)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:843)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-05-21 17:55:20,674 WARN hdfs.DataStreamer: Caught exception(176 + 1) / 200]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n",
      "2023-05-21 17:55:20,821 WARN hdfs.DataStreamer: Caught exception(177 + 1) / 200]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n",
      "2023-05-21 17:55:21,109 WARN hdfs.DataStreamer: Caught exception(179 + 1) / 200]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeInternal(DataStreamer.java:847)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:843)\n",
      "2023-05-21 17:55:22,210 WARN hdfs.DataStreamer: Caught exception(184 + 1) / 200]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n",
      "2023-05-21 17:55:23,462 WARN hdfs.DataStreamer: Caught exception(197 + 1) / 200]\n",
      "java.lang.InterruptedException\n",
      "\tat java.base/java.lang.Object.wait(Native Method)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1300)\n",
      "\tat java.base/java.lang.Thread.join(Thread.java:1375)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.closeResponder(DataStreamer.java:986)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.endBlock(DataStreamer.java:640)\n",
      "\tat org.apache.hadoop.hdfs.DataStreamer.run(DataStreamer.java:810)\n",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# save the file to a csv file\n",
    "weather_tweets2.write.csv(\"hdfs://localhost:9000/user1/weather_tweets2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74938fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddbb1848",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207715cd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d08f0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
